---
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# STATS 765, Lab 05
## Lab 05 Overview

We discussed lasso and ridge regressions in the last lecture. In this lab, we use simulated data sets to fit these models and compare their performances to illustrate when one works better.

## Task 1: Generate simulated data sets

To understand how a statistical learning method works, it is important to simulate a mock-up data set where you know the underlying structure, so that you can use the data set to examine and validate the behavior of a model.

### Step 1.1: Generate the data sets

Run the following code chunk. As a warm-up, what do you expect the output would be when you run lm(Y~. data = dat_A)

```{r}
# load libraries
library(glmnet)
library(MASS)
library(tidyverse)
# set up parameters
set.seed(765) 
n <- 80  
p <- 80  
lambdas <- 2^ seq(6, -4, length = 100)  # lambda values.
# set up correlation matrix between Xs.
cor.mat <- matrix(0.7, nrow = p, ncol = p)
diag(cor.mat) <- 1
cor.mat[lower.tri(cor.mat)] <- t(cor.mat)[lower.tri(cor.mat)]
# dat_A
# moderately correlated explanatory variables
# only the first five observations of x are related to y. The rest are noise
X1 <- mvrnorm(n, mu=rep(1, p), Sigma = cor.mat)
colnames(X1) <- paste0('X', str_pad(1:p, nchar(p), 'left', '0'))
n_betas <- 5
betas <- as.vector(scale(sample(1:n_betas))) * 10
related.ind <- sample(1:p, n_betas)
y1 <- as.numeric(betas %*% t(X1[,related.ind]) + rnorm(n))
dat_A <- cbind(data.frame(Y = y1), X1)
# dat_B
# moderately correlated explanatory variables
# all the explanatory variables have a relationship with y
X2 <- mvrnorm(n, mu=rep(1, p), Sigma = cor.mat)
colnames(X2) <- paste0('X', str_pad(1:p, nchar(p), 'left', '0'))
all_betas <- sample(as.vector(scale(1:p))) * 0.1
y2 <- as.numeric(all_betas %*% t(X2) + rnorm(n))
dat_B <- cbind(data.frame(Y = y2), X2)
```

I would expect an output of coefficients and intercepts for the 80 explanatory variables. I would expect that 5 of the 80 explanatory variables show a strong beta coefficient value as only five of those explanatory variables are related to Y when generating Y, indicating a strong correlation. The rest of the explanatory variables should be smaller in value as y has no correlation with them, however some variables might have higher beta values due to the moderate correlation of 0.7 we instantiated.

### Step 1.2: Understand the code
Examine the code chunk from 1.1 above, what is the similarity and difference between the two data sets, dat_A and dat_B?

Both datasets A and B, all 80 explanatory variables having moderate correlation. 5 random explanatory variables of Dat_A has a trong direct relation to Y, whereas the other explanatory variables weren't used to generate Y. Whereas dat_B has all 80 explanatory variables used to generate Y, however the relationship is relatively weak.

### Step 1.3: Split into train and test
Split each one of the data sets into training (80%) and test (20%) sets. You should have train_A, test_A, train_B, and test_B by the end of this step.

```{r}
# 80/20 train/test split for data_A
sample = sample(c(TRUE, FALSE), nrow(dat_A), replace=TRUE, prob=c(0.8,0.2))
train_A = as.matrix(dat_A[sample, ])
test_A = as.matrix(dat_A[!sample, ])

# 80/20 train/test split for data_B
sample = sample(c(TRUE, FALSE), nrow(dat_B), replace=TRUE, prob=c(0.8,0.2))
train_B = as.matrix(dat_B[sample, ])
test_B = as.matrix(dat_B[!sample, ])
```

## Task 2: Fit lasso regression
For each one of the data sets, do following:

fit a lasso model on the training set, with λ chosen by cross-validation.
Calculate the test MSPE, and the number of non-zero coefficient estimates from the optimal model.

```{r}
myFit = function(train_df, test_df, a = 0) { 
  # lasso: a = 1, ridge: a = 0
  train_y = train_df[,1]
  train_X = train_df[,-1]
  test_y = test_df[,1]
  test_X = test_df[,-1]
  
  cv_fit = cv.glmnet(train_X, train_y, alpha = a, lambda = lambdas)
  
  opt_lambda = cv_fit$lambda.min
  opt_fit = cv_fit$glmnet.fit
  betas = as.matrix(coef(opt_fit, s = cv_fit$lambda.min))
  n_non0_betas = sum(betas!=0)
  
  pred_y = predict(opt_fit, opt_lambda, newx = test_X)
  mse = mean((test_y - pred_y)^2)
  
  return(list(alpha = a, mse = mse, opt_lambda = opt_lambda, n_non0_betas = n_non0_betas))
}
```

```{r}
# for lasso
# data_A
result_A_lasso = myFit(train_A, test_A, 1)
# data_B
result_B_lasso = myFit(train_B, test_B, 1)
```

## Task 3: Fit ridge regression
For each one of the data sets, do following:

fit a ridge model on the training set, with λ chosen by cross-validation.
Calculate the test MSPE, and the number of non-zero coefficient estimates from the optimal model.

```{r}
# for ridge
# data_A
result_A_ridge = myFit(train_A, test_A, 0)
# data_B
result_B_ridge = myFit(train_B, test_B, 0)
```

## Task 4: Compare and comment
Compare all the results from above (that is, two models for each of the two data sets), and comment on their performance.

Lasso makes final model for data A sparse as most of the variables do not correlate to y and are just noise, this should leave us with far fewer variables with coefficients greater than 0, we also get shrinkage as the variable's coefficient's also shrink closer to 0. On the other hand, ridge regression just shrinks all variable coefficients for data A down close to 0. I would select Lasso in this case as out dataset has a lot of variables which don't correlate to y.

Ridge performs better for data B as all explanatory variables correlate to Y. Variable selection should be done by shrinkage of beta coefficients rather than driving some beta coefficients to 0 when using Lasso, as most if not all our variables are useful for predicting Y.



